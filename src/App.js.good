import React, { useState, useEffect, useRef } from 'react';
import './App.css';

function GroqApp() {
  const [textInput, setTextInput] = useState('');
  const [messages, setMessages] = useState([]);
  const [tokensPerSecond, setTokensPerSecond] = useState(0); // Placeholder
  const [isListening, setIsListening] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);

  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const animationFrameIdRef = useRef(null);
  const recordingTimeRef = useRef(null);

  useEffect(() => {
    return () => {
      cancelAnimationFrame(animationFrameIdRef.current);
      stopListening();
    };
  }, []);

  const startListening = () => {
    if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();

      recognition.onstart = () => {
        setIsListening(true);
        setRecordingTime(0);
        startRecordingTimer();
        startAudioVisualization();
      };

      recognition.onresult = (event) => {
        const transcript = event.results[0][0].transcript;
        setTextInput(transcript);
        console.log('Transcript:', transcript);
      };

      recognition.onend = () => {
        stopListening(); 
      };

      recognition.start();

    } else {
      alert('Speech recognition is not supported in your browser.');
    }
  };

  const stopListening = () => {
    setIsListening(false);
    stopRecordingTimer();
    stopAudioVisualization();
  };

  const startRecordingTimer = () => {
    const startTime = Date.now();
    recordingTimeRef.current = setInterval(() => {
      const elapsedTime = Date.now() - startTime;
      setRecordingTime(Math.floor(elapsedTime / 1000));
    }, 1000);
  };

  const stopRecordingTimer = () => {
    clearInterval(recordingTimeRef.current);
    setRecordingTime(0);
  };

  const startAudioVisualization = () => {
    audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
    analyserRef.current = audioContextRef.current.createAnalyser();

    navigator.mediaDevices.getUserMedia({ audio: true })
      .then(stream => {
        const source = audioContextRef.current.createMediaStreamSource(stream);
        source.connect(analyserRef.current);
        visualize();
      })
      .catch(err => {
        console.error('Error accessing microphone:', err);
      });
  };

  const visualize = () => {
    const canvas = document.getElementById('analyzer');
    const ctx = canvas.getContext('2d');
    const bufferLength = analyserRef.current.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const WIDTH = canvas.width;
    const HEIGHT = canvas.height;

    analyserRef.current.getByteTimeDomainData(dataArray);

    ctx.clearRect(0, 0, WIDTH, HEIGHT); // Clear the canvas
    ctx.fillStyle = 'rgb(255, 255, 255)'; 
    ctx.fillRect(0, 0, WIDTH, HEIGHT); 

    ctx.lineWidth = 2;
    ctx.strokeStyle = '#f0544f';
    ctx.beginPath();

    let sliceWidth = WIDTH * 1.0 / bufferLength;
    let x = 0;

    for (let i = 0; i < bufferLength; i++) {
      let v = dataArray[i] / 128.0;
      let y = v * HEIGHT / 2;

      if (i === 0) {
        ctx.moveTo(x, y);
      } else {
        ctx.lineTo(x, y);
      }

      x += sliceWidth;
    }

    ctx.lineTo(canvas.width, canvas.height / 2); 
    ctx.stroke();

    animationFrameIdRef.current = requestAnimationFrame(visualize);
  };

  const stopAudioVisualization = () => {
    cancelAnimationFrame(animationFrameIdRef.current);
    if (audioContextRef.current && audioContextRef.current.state === 'running') {
      audioContextRef.current.close(); 
    }
  };

  const handleSend = () => {
    if (textInput.trim() === '') return;

    const newMessage = {
      text: textInput,
      isUser: true
    };

    setMessages(prevMessages => [...prevMessages, newMessage]);
    setTextInput(''); 

    // TODO: Send newMessage.text to your backend API 
    // ... and handle the response (e.g., display it as a bot message)
  };

  return ( 
    <div className="app-container">
      <div className="input-area">
        <div className="input-container">
          <button onClick={isListening ? stopListening : startListening}>
            <div className={`microphone-icon ${isListening ? 'recording' : ''}`}>
              üé§
            </div>
            {isListening && <span className="recording-time">{recordingTime}</span>}
          </button>
          <input
            type="text"
            value={textInput}
            onChange={(e) => setTextInput(e.target.value)}
            placeholder="Try it"
          />
          <button onClick={handleSend}>
            <span role="img" aria-label="Send">
              ‚úàÔ∏è
            </span>
          </button>
        </div>
        <div className="options">
          <span>Clear chat</span>
          <span>
            <span role="img" aria-label="Arrows">
              ‚ÜîÔ∏è
            </span>{' '}
            Expand Width
          </span>
          <span className="tokens-per-second">
            {tokensPerSecond} T/s{' '}
            <span role="img" aria-label="Lightning">
              ‚ö°Ô∏è
            </span>
          </span>
        </div>
      </div>

      <div className="message-area">
        {messages.map((message, index) => (
          <div
            key={index}
            className={`message ${message.isUser ? 'user-message' : 'bot-message'
              }`}
          >
            {message.text}
          </div>
        ))}
      </div>

      <div className="extra-options">
        <button>
          <span role="img" aria-label="Square">
            ‚èπÔ∏è
          </span>
        </button>
        <button>
          <span role="img" aria-label="Right Arrow">
            ‚û°Ô∏è
          </span>
        </button>
        <button>
          <span role="img" aria-label="Lines">
            ‚ò∞
          </span>
        </button>
      </div>
    </div>
  );
}

export default GroqApp;